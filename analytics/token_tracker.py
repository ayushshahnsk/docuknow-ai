"""
Token Tracker for DocuKnow AI

Purpose:
- Track input, output, and total tokens per message
- Use an industry-accepted approximation
- Work with local LLMs like Gemma (via Ollama)

Note:
Local LLMs do NOT return token usage,
so we estimate:
1 token â‰ˆ 4 characters
"""

class TokenTracker:
    def __init__(self):
        self.total_input_tokens = 0
        self.total_output_tokens = 0

    # -----------------------------
    # Internal helper
    # -----------------------------
    @staticmethod
    def _estimate_tokens(text: str) -> int:
        """
        Approximate token count from text length.
        """
        if not text:
            return 0
        return max(1, len(text) // 4)

    # -----------------------------
    # Public API
    # -----------------------------
    def count_input(self, question: str, context: str) -> int:
        """
        Count input tokens for one request.
        Input = user question + retrieved context
        """
        combined = f"{question}\n{context}"
        tokens = self._estimate_tokens(combined)
        self.total_input_tokens += tokens
        return tokens

    def count_output(self, answer: str) -> int:
        """
        Count output tokens generated by the model.
        """
        tokens = self._estimate_tokens(answer)
        self.total_output_tokens += tokens
        return tokens

    def get_totals(self) -> dict:
        """
        Get cumulative token usage.
        """
        return {
            "input": self.total_input_tokens,
            "output": self.total_output_tokens,
            "total": self.total_input_tokens + self.total_output_tokens
        }

    def reset(self):
        """
        Reset token counters (used when chat is cleared).
        """
        self.total_input_tokens = 0
        self.total_output_tokens = 0
